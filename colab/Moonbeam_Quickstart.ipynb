{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moonbeam Quickstart (Google Colab, free GPU)\n",
    "\n",
    "This notebook runs **end-to-end inference** with the pretrained **Moonbeam 309M** checkpoint and writes `out.mid` with **no dataset and no finetuning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Runtime setup\n",
    "In Colab: `Runtime -> Change runtime type -> GPU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "print(subprocess.check_output([\"nvidia-smi\"], text=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Clone repo and install dependencies (exact README commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/guozixunnicolas/Moonbeam-MIDI-Foundation-Model.git\"\n",
    "REPO_DIR = \"/content/Moonbeam-MIDI-Foundation-Model\"\n",
    "\n",
    "if not os.path.isdir(REPO_DIR):\n",
    "    !git clone $REPO_URL $REPO_DIR\n",
    "else:\n",
    "    print(\"Repo already exists; syncing to latest origin/main (fallback origin/master).\")\n",
    "    !git -C $REPO_DIR fetch origin\n",
    "    !git -C $REPO_DIR reset --hard origin/main || git -C $REPO_DIR reset --hard origin/master\n",
    "\n",
    "%cd /content/Moonbeam-MIDI-Foundation-Model\n",
    "!pip install .\n",
    "!pip install src/llama_recipes/transformers_minimal/.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Download pretrained checkpoint from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "ckpt_path = hf_hub_download(\n",
    "    repo_id=\"guozixunnicolas/moonbeam-midi-foundation-model\",\n",
    "    filename=\"moonbeam_309M.pt\",\n",
    "    local_dir=\"checkpoints/pretrained\",\n",
    ")\n",
    "print(\"checkpoint:\", ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b) (Optional) Upload a LoRA adapter for generation\n",
    "If you trained a LoRA separately, upload a `.zip` containing `adapter_config.json` + adapter weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional LoRA upload controls\n",
    "USE_UPLOADED_LORA = False  #@param {type:\"boolean\"}\n",
    "UPLOADED_LORA_ZIP = \"\"  #@param {type:\"string\"}\n",
    "FINETUNED_PEFT_WEIGHT_PATH = None\n",
    "\n",
    "if USE_UPLOADED_LORA:\n",
    "    from google.colab import files\n",
    "    import zipfile\n",
    "    from pathlib import Path\n",
    "\n",
    "    if UPLOADED_LORA_ZIP.strip():\n",
    "        zip_path = Path(UPLOADED_LORA_ZIP)\n",
    "    else:\n",
    "        uploaded = files.upload()\n",
    "        if not uploaded:\n",
    "            raise RuntimeError(\"No LoRA zip uploaded.\")\n",
    "        zip_path = Path(next(iter(uploaded.keys())))\n",
    "\n",
    "    out_dir = Path(\"uploaded_lora\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "        zf.extractall(out_dir)\n",
    "\n",
    "    # find adapter root folder\n",
    "    candidates = [d for d in [out_dir, *out_dir.rglob('*')] if d.is_dir() and (d / 'adapter_config.json').exists()]\n",
    "    if not candidates:\n",
    "        raise RuntimeError(\"Could not find adapter_config.json in uploaded LoRA zip.\")\n",
    "    FINETUNED_PEFT_WEIGHT_PATH = str(candidates[0].resolve())\n",
    "    print(f\"Using uploaded LoRA adapter: {FINETUNED_PEFT_WEIGHT_PATH}\")\n",
    "else:\n",
    "    print(\"LoRA disabled. Set USE_UPLOADED_LORA=True to upload/apply adapter.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Resolve config + tokenizer paths used by repo\n",
    "- Model config: `src/llama_recipes/configs/model_config.json`\n",
    "- Tokenizer: search for `tokenizer.model` in repo, fallback to benchmark tokenizer path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import subprocess\n",
    "import torch\n",
    "\n",
    "repo_root = Path.cwd()\n",
    "primary_model_config_path = repo_root / \"src/llama_recipes/configs/model_config.json\"\n",
    "small_model_config_path = repo_root / \"src/llama_recipes/configs/model_config_small.json\"\n",
    "assert primary_model_config_path.exists(), f\"Missing model config: {primary_model_config_path}\"\n",
    "\n",
    "# Search for tokenizer.model in repo.\n",
    "search = subprocess.run(\n",
    "    [\"bash\", \"-lc\", \"rg --files | rg 'tokenizer\\.model$'\"],\n",
    "    cwd=repo_root,\n",
    "    text=True,\n",
    "    capture_output=True,\n",
    "    check=False,\n",
    ")\n",
    "found = [line.strip() for line in search.stdout.splitlines() if line.strip()]\n",
    "print(\"tokenizer.model candidates:\", found)\n",
    "\n",
    "if found:\n",
    "    tokenizer_path = repo_root / found[0]\n",
    "else:\n",
    "    tokenizer_path = repo_root / \"recipes/benchmarks/inference_throughput/tokenizer/tokenizer.model\"\n",
    "\n",
    "assert tokenizer_path.exists(), f\"Missing tokenizer file: {tokenizer_path}\"\n",
    "\n",
    "# Detect which config matches checkpoint tensor shapes (309M checkpoint expects *_small config).\n",
    "resolved_model_config_path = primary_model_config_path\n",
    "if 'ckpt_path' in globals() and small_model_config_path.exists():\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "    state = checkpoint.get('model_state_dict') if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint else checkpoint\n",
    "    if isinstance(state, dict):\n",
    "        normalized_state = {k[7:] if k.startswith('module.') else k: v for k, v in state.items()}\n",
    "        norm_key = 'model.norm.weight'\n",
    "        ckpt_hidden_size = normalized_state.get(norm_key).shape[0] if norm_key in normalized_state else None\n",
    "        if ckpt_hidden_size is not None:\n",
    "            with open(primary_model_config_path) as f:\n",
    "                primary_hidden = json.load(f).get('hidden_size')\n",
    "            with open(small_model_config_path) as f:\n",
    "                small_hidden = json.load(f).get('hidden_size')\n",
    "            if ckpt_hidden_size == small_hidden and ckpt_hidden_size != primary_hidden:\n",
    "                resolved_model_config_path = small_model_config_path\n",
    "\n",
    "print(\"using model_config_path:\", resolved_model_config_path)\n",
    "print(\"using tokenizer_path:\", tokenizer_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Add dataset-free inference entrypoint (SOS-only prompt)\n",
    "This avoids the existing CSV + `.npy` prompt requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we are in the cloned repo (some Colab workflows can change cwd).\n",
    "%cd /content/Moonbeam-MIDI-Foundation-Model\n",
    "\n",
    "from pathlib import Path\n",
    "import textwrap\n",
    "\n",
    "entrypoint = Path(\"recipes/inference/custom_music_generation/unconditional_from_scratch.py\")\n",
    "generation_impl = Path(\"recipes/inference/custom_music_generation/generation.py\")\n",
    "\n",
    "# Some upstream/older clones do not include this helper entrypoint yet.\n",
    "# If missing, bootstrap a compatible script so the notebook remains runnable.\n",
    "if not entrypoint.exists():\n",
    "    print(f\"[info] Missing {entrypoint}; creating compatibility entrypoint.\")\n",
    "    entrypoint.parent.mkdir(parents=True, exist_ok=True)\n",
    "    entrypoint.write_text(textwrap.dedent('''\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "\n",
    "from generation import MusicLlama\n",
    "from llama_recipes.datasets.music_tokenizer import MusicTokenizer\n",
    "\n",
    "\n",
    "def _normalize_checkpoint_state_dict(checkpoint: dict) -> dict:\n",
    "    if not isinstance(checkpoint, dict):\n",
    "        raise ValueError(\"Checkpoint must be a dict-like object.\")\n",
    "    if \"model_state_dict\" in checkpoint and isinstance(checkpoint[\"model_state_dict\"], dict):\n",
    "        state_dict = checkpoint[\"model_state_dict\"]\n",
    "    elif \"state_dict\" in checkpoint and isinstance(checkpoint[\"state_dict\"], dict):\n",
    "        state_dict = checkpoint[\"state_dict\"]\n",
    "    elif \"model\" in checkpoint and isinstance(checkpoint[\"model\"], dict):\n",
    "        state_dict = checkpoint[\"model\"]\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    return {k[7:] if k.startswith(\"module.\") else k: v for k, v in state_dict.items()}\n",
    "\n",
    "\n",
    "def _build_music_llama(ckpt_path: str, model_config_path: str) -> MusicLlama:\n",
    "    config = LlamaConfig.from_pretrained(model_config_path)\n",
    "    model = LlamaForCausalLM(config)\n",
    "    try:\n",
    "        checkpoint = torch.load(ckpt_path, map_location=\"cpu\", weights_only=True)\n",
    "    except TypeError:\n",
    "        checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    state_dict = _normalize_checkpoint_state_dict(checkpoint)\n",
    "    model_state = model.state_dict()\n",
    "    missing = sorted(set(model_state.keys()) - set(state_dict.keys()))\n",
    "    unexpected = sorted(set(state_dict.keys()) - set(model_state.keys()))\n",
    "    shape_mismatch = sorted([k for k in (set(state_dict.keys()) & set(model_state.keys())) if getattr(state_dict[k], \"shape\", None) != model_state[k].shape])\n",
    "    if missing or unexpected or shape_mismatch:\n",
    "        raise RuntimeError(\n",
    "            f\"Checkpoint/config mismatch (strict mode): missing={len(missing)}, unexpected={len(unexpected)}, shape_mismatch={len(shape_mismatch)}\"\n",
    "        )\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    model = model.to(\"cuda\")\n",
    "    if torch.cuda.is_bf16_supported():\n",
    "        model = model.to(torch.bfloat16)\n",
    "    model.eval()\n",
    "    tokenizer = MusicTokenizer(\n",
    "        timeshift_vocab_size=config.onset_vocab_size,\n",
    "        dur_vocab_size=config.dur_vocab_size,\n",
    "        octave_vocab_size=config.octave_vocab_size,\n",
    "        pitch_class_vocab_size=config.pitch_class_vocab_size,\n",
    "        instrument_vocab_size=config.instrument_vocab_size,\n",
    "        velocity_vocab_size=config.velocity_vocab_size,\n",
    "    )\n",
    "    return MusicLlama(model, tokenizer, config)\n",
    "\n",
    "\n",
    "def main(\n",
    "    ckpt_path: str,\n",
    "    model_config_path: str = \"src/llama_recipes/configs/model_config.json\",\n",
    "    output_midi_path: str = \"out.mid\",\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 0.95,\n",
    "    max_gen_len: int = 512,\n",
    "    seed: int = 42,\n",
    "    num_samples: int = 1,\n",
    ") -> None:\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA GPU is required.\")\n",
    "    generator = _build_music_llama(ckpt_path=ckpt_path, model_config_path=model_config_path)\n",
    "    output_path = Path(output_midi_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    for i in range(max(1, int(num_samples))):\n",
    "        sample_seed = int(seed) + i\n",
    "        torch.manual_seed(sample_seed)\n",
    "        torch.cuda.manual_seed_all(sample_seed)\n",
    "        out = generator.music_completion(\n",
    "            prompt_tokens=[[generator.tokenizer.sos_token_compound]],\n",
    "            temperature=float(temperature),\n",
    "            top_p=float(top_p),\n",
    "            max_gen_len=int(max_gen_len),\n",
    "        )[0][\"generation\"][\"tokens\"]\n",
    "        valid = []\n",
    "        for row in out:\n",
    "            if len(row) == 6 and all(isinstance(x, (int, float)) for x in row):\n",
    "                valid.append([int(x) for x in row])\n",
    "        if not valid:\n",
    "            raise RuntimeError(\"No valid generated rows.\")\n",
    "        path = output_path if int(num_samples) == 1 else output_path.with_name(f\"{output_path.stem}_{i+1}{output_path.suffix}\")\n",
    "        generator.tokenizer.compound_to_midi(valid).save(str(path))\n",
    "        print(f\"Saved {path.resolve()} with {len(valid)} tokens\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(main)\n",
    "'''))\n",
    "\n",
    "assert entrypoint.exists(), f\"Failed to create entrypoint: {entrypoint}\"\n",
    "assert generation_impl.exists(), f\"Missing generation implementation: {generation_impl}\"\n",
    "print(f\"Using repo entrypoint: {entrypoint.resolve()}\")\n",
    "print(f\"Using generator implementation: {generation_impl.resolve()}\")\n",
    "\n",
    "# Auto-heal older copies in Colab if EOS handling is stale, then verify.\n",
    "gen_src = generation_impl.read_text()\n",
    "if \"torch.any(eos_conditions_all_attr\" in gen_src and \"torch.all(eos_conditions_all_attr\" not in gen_src:\n",
    "    print(\"[info] Patching EOS handling in generation.py to require full EOS compound token...\")\n",
    "    gen_src = gen_src.replace(\"torch.any(eos_conditions_all_attr, dim = -1)\", \"torch.all(eos_conditions_all_attr, dim = -1)\")\n",
    "    generation_impl.write_text(gen_src)\n",
    "\n",
    "gen_src = generation_impl.read_text()\n",
    "assert \"torch.all(eos_conditions_all_attr\" in gen_src, \"generation.py is missing full-EOS handling fix.\"\n",
    "\n",
    "!python recipes/inference/custom_music_generation/unconditional_from_scratch.py --help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Recommended default flow: MIDI continuation (prompted generation)\n",
    "Continuation is typically more structured and musically coherent than pure from-scratch sampling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality presets\n",
    "Choose a preset to adjust sampling only (no weight changes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quality_presets"
   },
   "outputs": [],
   "source": [
    "QUALITY_PRESET = \"balanced\"  #@param [\"conservative\",\"balanced\",\"creative\"]\n",
    "USE_RANDOM_SEED = False  #@param {type:\"boolean\"}\n",
    "BASE_SEED_DEFAULT = 42\n",
    "\n",
    "PRESETS = {\n",
    "    \"conservative\": {\"temperature\": 0.85, \"top_p\": 0.90, \"scratch_len\": 384, \"cont_len\": 256},\n",
    "    \"balanced\": {\"temperature\": 0.95, \"top_p\": 0.93, \"scratch_len\": 512, \"cont_len\": 384},\n",
    "    \"creative\": {\"temperature\": 1.10, \"top_p\": 0.98, \"scratch_len\": 640, \"cont_len\": 512},\n",
    "}\n",
    "preset = PRESETS[QUALITY_PRESET]\n",
    "if USE_RANDOM_SEED:\n",
    "    import random\n",
    "    BASE_SEED_DEFAULT = random.randint(0, 2**31 - 1)\n",
    "\n",
    "print(\"Selected quality preset:\")\n",
    "print(f\"  preset={QUALITY_PRESET} temp={preset['temperature']} top_p={preset['top_p']} scratch_len={preset['scratch_len']} cont_len={preset['cont_len']} base_seed={BASE_SEED_DEFAULT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: choose how many songs to generate\nSet `NUM_GENERATIONS` to 1, 2, 3, 4, etc., and rerun cell 6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional advanced mode: from-scratch generation controls\n",
    "ADVANCED_RUN_SCRATCH = False  #@param {type:\"boolean\"}\n",
    "NUM_GENERATIONS = 1  #@param {type:\"integer\"}\n",
    "BASE_SEED = int(globals().get(\"BASE_SEED_DEFAULT\", 42))  #@param {type:\"integer\"}\n",
    "VARIATION_OFFSET = 0  #@param {type:\"integer\"}\n",
    "MAX_GEN_LEN = int(globals().get(\"preset\", {}).get(\"scratch_len\", 512))  #@param {type:\"integer\"}\n",
    "TEMPERATURE = float(globals().get(\"preset\", {}).get(\"temperature\", 0.95))  #@param {type:\"number\"}\n",
    "TOP_P = float(globals().get(\"preset\", {}).get(\"top_p\", 0.93))  #@param {type:\"number\"}\n",
    "print(f\"Scratch advanced mode -> enabled={ADVANCED_RUN_SCRATCH}, num={NUM_GENERATIONS}, base_seed={BASE_SEED}, max_gen_len={MAX_GEN_LEN}, temperature={TEMPERATURE}, top_p={TOP_P}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/Moonbeam-MIDI-Foundation-Model\n",
    "\n",
    "from pathlib import Path\n",
    "import gc, io, contextlib, sys\n",
    "import torch\n",
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "sys.path.insert(0, str(Path(\"recipes/inference/custom_music_generation\").resolve()))\n",
    "from generation import MusicLlama\n",
    "from llama_recipes.datasets.music_tokenizer import MusicTokenizer\n",
    "\n",
    "if not hasattr(MusicTokenizer, \"_orig_convert_from_language_tokens\"):\n",
    "    MusicTokenizer._orig_convert_from_language_tokens = MusicTokenizer.convert_from_language_tokens\n",
    "def _convert_from_language_tokens_on_device(self, inp):\n",
    "    out = MusicTokenizer._orig_convert_from_language_tokens(self, inp)\n",
    "    return out.to(inp.device) if torch.is_tensor(inp) else out\n",
    "MusicTokenizer.convert_from_language_tokens = _convert_from_language_tokens_on_device\n",
    "\n",
    "def _normalize_checkpoint_state_dict(checkpoint: dict) -> dict:\n",
    "    if not isinstance(checkpoint, dict):\n",
    "        raise ValueError(\"Checkpoint must be dict-like.\")\n",
    "    if \"model_state_dict\" in checkpoint and isinstance(checkpoint[\"model_state_dict\"], dict):\n",
    "        state_dict = checkpoint[\"model_state_dict\"]\n",
    "    elif \"state_dict\" in checkpoint and isinstance(checkpoint[\"state_dict\"], dict):\n",
    "        state_dict = checkpoint[\"state_dict\"]\n",
    "    elif \"model\" in checkpoint and isinstance(checkpoint[\"model\"], dict):\n",
    "        state_dict = checkpoint[\"model\"]\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    return {k[7:] if k.startswith(\"module.\") else k: v for k, v in state_dict.items()}\n",
    "\n",
    "def _strict_load_or_fail(model, state_dict):\n",
    "    model_state = model.state_dict()\n",
    "    missing = sorted(set(model_state.keys()) - set(state_dict.keys()))\n",
    "    unexpected = sorted(set(state_dict.keys()) - set(model_state.keys()))\n",
    "    shape_mismatch = sorted([\n",
    "        k for k in (set(state_dict.keys()) & set(model_state.keys()))\n",
    "        if getattr(state_dict[k], \"shape\", None) != model_state[k].shape\n",
    "    ])\n",
    "    if missing or unexpected or shape_mismatch:\n",
    "        def _p(keys):\n",
    "            return \", \".join(keys[:8]) + (\" ...\" if len(keys) > 8 else \"\")\n",
    "        raise RuntimeError(\n",
    "            \"Checkpoint/config mismatch detected (strict mode). \"\n",
    "            f\"missing={len(missing)} ({_p(missing)}), \"\n",
    "            f\"unexpected={len(unexpected)} ({_p(unexpected)}), \"\n",
    "            f\"shape_mismatch={len(shape_mismatch)} ({_p(shape_mismatch)}). \"\n",
    "            \"Use matching checkpoint + config (or model_config_small.json for small checkpoints).\"\n",
    "        )\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "def _sanitize_tokens(tokenizer, rows):\n",
    "    # Minimal validation only: drop invalid rows, do not rewrite values.\n",
    "    cleaned, last_onset = [], -1\n",
    "    max_onset = max(0, tokenizer.timeshift_vocab_size - 3)\n",
    "    max_dur = max(0, tokenizer.dur_vocab_size - 3)\n",
    "    max_oct = max(0, tokenizer.octave_vocab_size - 3)\n",
    "    max_pitch = max(0, tokenizer.pitch_class_vocab_size - 3)\n",
    "    max_instr = max(0, tokenizer.instrument_vocab_size - 3)\n",
    "    max_vel = max(0, tokenizer.velocity_vocab_size - 3)\n",
    "    for row in rows:\n",
    "        if len(row) != 6:\n",
    "            continue\n",
    "        onset, duration, octave, pitch, instrument, velocity = [int(x) for x in row]\n",
    "        if onset < 0 or onset > max_onset:\n",
    "            continue\n",
    "        if duration <= 0 or duration > max_dur:\n",
    "            continue\n",
    "        if onset < last_onset:\n",
    "            continue\n",
    "        if not (0 <= octave <= max_oct):\n",
    "            continue\n",
    "        if not (0 <= pitch <= max_pitch):\n",
    "            continue\n",
    "        if not (0 <= instrument <= max_instr):\n",
    "            continue\n",
    "        if not (0 <= velocity <= max_vel):\n",
    "            continue\n",
    "        cleaned.append([onset, duration, octave, pitch, instrument, velocity])\n",
    "        last_onset = onset\n",
    "    return cleaned\n",
    "\n",
    "for var_name in [\"generator\", \"model\", \"checkpoint\", \"state_dict\", \"tokenizer\"]:\n",
    "    if var_name in globals():\n",
    "        del globals()[var_name]\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA GPU is required. In Colab set Runtime -> GPU.\")\n",
    "\n",
    "config = LlamaConfig.from_pretrained(str(resolved_model_config_path))\n",
    "model = LlamaForCausalLM(config)\n",
    "try:\n",
    "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\", weights_only=True)\n",
    "except TypeError:\n",
    "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "state_dict = _normalize_checkpoint_state_dict(checkpoint)\n",
    "_strict_load_or_fail(model, state_dict)\n",
    "print(f\"[info] Strict load successful: {len(state_dict)} tensors\")\n",
    "\n",
    "if FINETUNED_PEFT_WEIGHT_PATH:\n",
    "    print(f\"Applying LoRA adapter from: {FINETUNED_PEFT_WEIGHT_PATH}\")\n",
    "    model = PeftModel.from_pretrained(model, FINETUNED_PEFT_WEIGHT_PATH, is_trainable=False)\n",
    "\n",
    "model = model.to(\"cuda\")\n",
    "if torch.cuda.is_bf16_supported():\n",
    "    model = model.to(torch.bfloat16)\n",
    "model.eval()\n",
    "\n",
    "with contextlib.redirect_stdout(io.StringIO()):\n",
    "    tokenizer = MusicTokenizer(\n",
    "        timeshift_vocab_size=config.onset_vocab_size,\n",
    "        dur_vocab_size=config.dur_vocab_size,\n",
    "        octave_vocab_size=config.octave_vocab_size,\n",
    "        pitch_class_vocab_size=config.pitch_class_vocab_size,\n",
    "        instrument_vocab_size=config.instrument_vocab_size,\n",
    "        velocity_vocab_size=config.velocity_vocab_size,\n",
    "    )\n",
    "generator = MusicLlama(model, tokenizer, config)\n",
    "print(\"Generator ready. Recommended next step: run continuation cells below.\")\n",
    "\n",
    "if bool(globals().get(\"ADVANCED_RUN_SCRATCH\", False)):\n",
    "    num_generations = max(1, int(globals().get(\"NUM_GENERATIONS\", 1)))\n",
    "    base_seed = int(globals().get(\"BASE_SEED\", 42))\n",
    "    variation_offset = int(globals().get(\"VARIATION_OFFSET\", 0))\n",
    "    max_gen_len = int(globals().get(\"MAX_GEN_LEN\", 512))\n",
    "    temperature = float(globals().get(\"TEMPERATURE\", 0.95))\n",
    "    top_p = float(globals().get(\"TOP_P\", 0.93))\n",
    "    all_outputs = []\n",
    "    for i in range(num_generations):\n",
    "        sample_seed = base_seed + variation_offset + i\n",
    "        torch.manual_seed(sample_seed)\n",
    "        torch.cuda.manual_seed_all(sample_seed)\n",
    "        sos_prompt = [generator.tokenizer.sos_token_compound]\n",
    "        result = generator.music_completion(\n",
    "            prompt_tokens=[sos_prompt],\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_gen_len=max_gen_len,\n",
    "        )[0]\n",
    "        sanitized_tokens = _sanitize_tokens(generator.tokenizer, result[\"generation\"][\"tokens\"])\n",
    "        if not sanitized_tokens:\n",
    "            raise RuntimeError(f\"No valid generated tokens remained after validation for sample {i}.\")\n",
    "        out_path = Path(\"out.mid\") if num_generations == 1 else Path(f\"out_{i+1}.mid\")\n",
    "        generator.tokenizer.compound_to_midi(sanitized_tokens).save(str(out_path))\n",
    "        print(f\"Saved MIDI to: {out_path.resolve()} | tokens={len(sanitized_tokens)} | seed={sample_seed}\")\n",
    "        all_outputs.append(out_path)\n",
    "    print(f\"Done. Generated {len(all_outputs)} from-scratch file(s).\")\n",
    "else:\n",
    "    print(\"From-scratch generation skipped (ADVANCED_RUN_SCRATCH=False).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6b) Continue from an uploaded MIDI (default path)\n",
    "Upload a MIDI, then generate continuations using the selected quality preset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "continuation_controls"
   },
   "outputs": [],
   "source": [
    "# Continuation controls (default flow)\n",
    "CONT_NUM_GENERATIONS = 2  #@param {type:\"integer\"}\n",
    "CONT_BASE_SEED = int(globals().get(\"BASE_SEED_DEFAULT\", 123))  #@param {type:\"integer\"}\n",
    "CONT_VARIATION_OFFSET = 0  #@param {type:\"integer\"}\n",
    "CONT_MAX_GEN_LEN = int(globals().get(\"preset\", {}).get(\"cont_len\", 384))  #@param {type:\"integer\"}\n",
    "CONT_TEMPERATURE = float(globals().get(\"preset\", {}).get(\"temperature\", 0.95))  #@param {type:\"number\"}\n",
    "CONT_TOP_P = float(globals().get(\"preset\", {}).get(\"top_p\", 0.93))  #@param {type:\"number\"}\n",
    "CONT_USE_FULL_PROMPT = True  #@param {type:\"boolean\"}\n",
    "CONT_PROMPT_MAX_TOKENS = 256  #@param {type:\"integer\"}\n",
    "print(f\"Continuation -> num={CONT_NUM_GENERATIONS}, base_seed={CONT_BASE_SEED}, max_gen_len={CONT_MAX_GEN_LEN}, temperature={CONT_TEMPERATURE}, top_p={CONT_TOP_P}, use_full_prompt={CONT_USE_FULL_PROMPT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "continuation_run"
   },
   "outputs": [],
   "source": [
    "%cd /content/Moonbeam-MIDI-Foundation-Model\n",
    "\n",
    "from pathlib import Path\n",
    "from google.colab import files\n",
    "import torch\n",
    "\n",
    "uploaded = files.upload()\n",
    "if not uploaded:\n",
    "    raise RuntimeError(\"No file uploaded. Upload a .mid file.\")\n",
    "\n",
    "upload_name = next(iter(uploaded.keys()))\n",
    "input_midi_path = Path(upload_name)\n",
    "print(f\"Uploaded: {input_midi_path}\")\n",
    "\n",
    "prompt_tokens = generator.tokenizer.midi_to_compound(str(input_midi_path))\n",
    "if not prompt_tokens:\n",
    "    raise RuntimeError(\"Uploaded MIDI produced an empty token list.\")\n",
    "\n",
    "use_full_prompt = bool(globals().get(\"CONT_USE_FULL_PROMPT\", True))\n",
    "prompt_max = max(1, int(globals().get(\"CONT_PROMPT_MAX_TOKENS\", 256)))\n",
    "if use_full_prompt:\n",
    "    prompt_tokens_for_gen = prompt_tokens\n",
    "else:\n",
    "    prompt_tokens_for_gen = prompt_tokens[-prompt_max:]\n",
    "\n",
    "num_generations = max(1, int(globals().get(\"CONT_NUM_GENERATIONS\", 1)))\n",
    "base_seed = int(globals().get(\"CONT_BASE_SEED\", 123))\n",
    "variation_offset = int(globals().get(\"CONT_VARIATION_OFFSET\", 0))\n",
    "max_gen_len = int(globals().get(\"CONT_MAX_GEN_LEN\", 384))\n",
    "temperature = float(globals().get(\"CONT_TEMPERATURE\", 0.9))\n",
    "top_p = float(globals().get(\"CONT_TOP_P\", 0.95))\n",
    "\n",
    "continuation_outputs = []\n",
    "for i in range(num_generations):\n",
    "    sample_seed = base_seed + variation_offset + i\n",
    "    torch.manual_seed(sample_seed)\n",
    "    torch.cuda.manual_seed_all(sample_seed)\n",
    "\n",
    "    result = generator.music_completion(\n",
    "        prompt_tokens=[prompt_tokens_for_gen],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_gen_len=max_gen_len,\n",
    "    )[0]\n",
    "\n",
    "    sanitized_tokens = _sanitize_tokens(generator.tokenizer, result[\"generation\"][\"tokens\"])\n",
    "    if not sanitized_tokens:\n",
    "        raise RuntimeError(f\"No valid continuation tokens remained after sanitization for sample {i}.\")\n",
    "\n",
    "    out_path = Path(f\"cont_{i+1}.mid\")\n",
    "    generator.tokenizer.compound_to_midi(sanitized_tokens).save(str(out_path))\n",
    "    print(f\"Saved continuation: {out_path.resolve()} | tokens={len(sanitized_tokens)} | seed={sample_seed}\")\n",
    "    continuation_outputs.append(out_path)\n",
    "\n",
    "print(f\"Used prompt tokens: {len(prompt_tokens_for_gen)} / original {len(prompt_tokens)}\")\n",
    "print(f\"Done. Generated {len(continuation_outputs)} continuation file(s).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Verify output and (optional) render to audio preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "outputs = sorted(Path('.').glob('out*.mid')) + sorted(Path('.').glob('cont_*.mid'))\n",
    "assert outputs, \"No MIDI outputs were created\"\n",
    "for out_path in outputs:\n",
    "    assert out_path.stat().st_size > 0, f\"{out_path} is empty\"\n",
    "print(\"âœ… Generated files:\")\n",
    "for out_path in outputs:\n",
    "    print(\" -\", out_path.resolve(), \"size:\", out_path.stat().st_size, \"bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional audio preview if dependencies are available.\n",
    "# If synthesis backends are unavailable in Colab, this cell may be skipped.\n",
    "\n",
    "!pip install pretty_midi midi2audio\n",
    "\n",
    "from pathlib import Path\n",
    "import pretty_midi\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "midi_files = sorted(Path('.').glob('out*.mid')) + sorted(Path('.').glob('cont_*.mid'))\n",
    "assert midi_files, \"No out*.mid/cont_*.mid files found. Run generation cell first.\"\n",
    "\n",
    "for midi_path in midi_files:\n",
    "    print(f\"Rendering: {midi_path}\")\n",
    "    midi = pretty_midi.PrettyMIDI(str(midi_path))\n",
    "    # Attempt software synthesis (requires fluidsynth backend in runtime)\n",
    "    audio = midi.synthesize(fs=16000)\n",
    "    display(Audio(audio, rate=16000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on checkpoint compatibility\n",
    "`MusicLlama.build()` in this fork was updated to accept multiple checkpoint layouts:\n",
    "- `{\"model_state_dict\": ...}`\n",
    "- `{\"state_dict\": ...}`\n",
    "- `{\"model\": ...}`\n",
    "- or a raw state dict\n",
    "\n",
    "It also strips `module.` prefixes when present."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
