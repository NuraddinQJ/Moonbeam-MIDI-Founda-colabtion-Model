{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moonbeam Quickstart (Google Colab, free GPU)\n",
    "\n",
    "This notebook runs **end-to-end inference** with the pretrained **Moonbeam 309M** checkpoint and writes `out.mid` with **no dataset and no finetuning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Runtime setup\n",
    "In Colab: `Runtime -> Change runtime type -> GPU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "print(subprocess.check_output([\"nvidia-smi\"], text=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Clone repo and install dependencies (exact README commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/guozixunnicolas/Moonbeam-MIDI-Foundation-Model.git\"\n",
    "REPO_DIR = \"/content/Moonbeam-MIDI-Foundation-Model\"\n",
    "\n",
    "if not os.path.isdir(REPO_DIR):\n",
    "    !git clone $REPO_URL $REPO_DIR\n",
    "else:\n",
    "    print(\"Repo already exists; syncing to latest origin/main (fallback origin/master).\")\n",
    "    !git -C $REPO_DIR fetch origin\n",
    "    !git -C $REPO_DIR reset --hard origin/main || git -C $REPO_DIR reset --hard origin/master\n",
    "\n",
    "%cd /content/Moonbeam-MIDI-Foundation-Model\n",
    "!pip install .\n",
    "!pip install src/llama_recipes/transformers_minimal/.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Download pretrained checkpoint from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "ckpt_path = hf_hub_download(\n",
    "    repo_id=\"guozixunnicolas/moonbeam-midi-foundation-model\",\n",
    "    filename=\"moonbeam_309M.pt\",\n",
    "    local_dir=\"checkpoints/pretrained\",\n",
    ")\n",
    "print(\"checkpoint:\", ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b) (Optional) Upload a LoRA adapter for generation\n",
    "If you trained a LoRA separately, upload a `.zip` containing `adapter_config.json` + adapter weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional LoRA upload controls\n",
    "USE_UPLOADED_LORA = False  #@param {type:\"boolean\"}\n",
    "UPLOADED_LORA_ZIP = \"\"  #@param {type:\"string\"}\n",
    "FINETUNED_PEFT_WEIGHT_PATH = None\n",
    "\n",
    "if USE_UPLOADED_LORA:\n",
    "    from google.colab import files\n",
    "    import zipfile\n",
    "    from pathlib import Path\n",
    "\n",
    "    if UPLOADED_LORA_ZIP.strip():\n",
    "        zip_path = Path(UPLOADED_LORA_ZIP)\n",
    "    else:\n",
    "        uploaded = files.upload()\n",
    "        if not uploaded:\n",
    "            raise RuntimeError(\"No LoRA zip uploaded.\")\n",
    "        zip_path = Path(next(iter(uploaded.keys())))\n",
    "\n",
    "    out_dir = Path(\"uploaded_lora\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "        zf.extractall(out_dir)\n",
    "\n",
    "    # find adapter root folder\n",
    "    candidates = [d for d in [out_dir, *out_dir.rglob('*')] if d.is_dir() and (d / 'adapter_config.json').exists()]\n",
    "    if not candidates:\n",
    "        raise RuntimeError(\"Could not find adapter_config.json in uploaded LoRA zip.\")\n",
    "    FINETUNED_PEFT_WEIGHT_PATH = str(candidates[0].resolve())\n",
    "    print(f\"Using uploaded LoRA adapter: {FINETUNED_PEFT_WEIGHT_PATH}\")\n",
    "else:\n",
    "    print(\"LoRA disabled. Set USE_UPLOADED_LORA=True to upload/apply adapter.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Resolve config + tokenizer paths used by repo\n",
    "- Model config: `src/llama_recipes/configs/model_config.json`\n",
    "- Tokenizer: search for `tokenizer.model` in repo, fallback to benchmark tokenizer path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import subprocess\n",
    "import torch\n",
    "\n",
    "repo_root = Path.cwd()\n",
    "primary_model_config_path = repo_root / \"src/llama_recipes/configs/model_config.json\"\n",
    "small_model_config_path = repo_root / \"src/llama_recipes/configs/model_config_small.json\"\n",
    "assert primary_model_config_path.exists(), f\"Missing model config: {primary_model_config_path}\"\n",
    "\n",
    "# Search for tokenizer.model in repo.\n",
    "search = subprocess.run(\n",
    "    [\"bash\", \"-lc\", \"rg --files | rg 'tokenizer\\.model$'\"],\n",
    "    cwd=repo_root,\n",
    "    text=True,\n",
    "    capture_output=True,\n",
    "    check=False,\n",
    ")\n",
    "found = [line.strip() for line in search.stdout.splitlines() if line.strip()]\n",
    "print(\"tokenizer.model candidates:\", found)\n",
    "\n",
    "if found:\n",
    "    tokenizer_path = repo_root / found[0]\n",
    "else:\n",
    "    tokenizer_path = repo_root / \"recipes/benchmarks/inference_throughput/tokenizer/tokenizer.model\"\n",
    "\n",
    "assert tokenizer_path.exists(), f\"Missing tokenizer file: {tokenizer_path}\"\n",
    "\n",
    "# Detect which config matches checkpoint tensor shapes (309M checkpoint expects *_small config).\n",
    "resolved_model_config_path = primary_model_config_path\n",
    "if 'ckpt_path' in globals() and small_model_config_path.exists():\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "    state = checkpoint.get('model_state_dict') if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint else checkpoint\n",
    "    if isinstance(state, dict):\n",
    "        normalized_state = {k[7:] if k.startswith('module.') else k: v for k, v in state.items()}\n",
    "        norm_key = 'model.norm.weight'\n",
    "        ckpt_hidden_size = normalized_state.get(norm_key).shape[0] if norm_key in normalized_state else None\n",
    "        if ckpt_hidden_size is not None:\n",
    "            with open(primary_model_config_path) as f:\n",
    "                primary_hidden = json.load(f).get('hidden_size')\n",
    "            with open(small_model_config_path) as f:\n",
    "                small_hidden = json.load(f).get('hidden_size')\n",
    "            if ckpt_hidden_size == small_hidden and ckpt_hidden_size != primary_hidden:\n",
    "                resolved_model_config_path = small_model_config_path\n",
    "\n",
    "print(\"using model_config_path:\", resolved_model_config_path)\n",
    "print(\"using tokenizer_path:\", tokenizer_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Add dataset-free inference entrypoint (SOS-only prompt)\n",
    "This avoids the existing CSV + `.npy` prompt requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we are in the cloned repo (some Colab workflows can change cwd).\n",
    "%cd /content/Moonbeam-MIDI-Foundation-Model\n",
    "\n",
    "from pathlib import Path\n",
    "entrypoint = Path(\"recipes/inference/custom_music_generation/unconditional_from_scratch.py\")\n",
    "assert entrypoint.exists(), f\"Missing entrypoint: {entrypoint}\"\n",
    "print(f\"Using repo entrypoint (no notebook overwrite): {entrypoint}\")\n",
    "\n",
    "!PYTHONPATH=src python recipes/inference/custom_music_generation/unconditional_from_scratch.py --help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Generate MIDI from scratch (no dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: choose how many songs to generate\nSet `NUM_GENERATIONS` to 1, 2, 3, 4, etc., and rerun cell 6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User controls for cell 6 (unconditional generation)\n",
    "NUM_GENERATIONS = 4  #@param {type:\"integer\"}\n",
    "BASE_SEED = 42  #@param {type:\"integer\"}\n",
    "VARIATION_OFFSET = 0  #@param {type:\"integer\"}\n",
    "MAX_GEN_LEN = 256  #@param {type:\"integer\"}\n",
    "TEMPERATURE = 0.9  #@param {type:\"number\"}\n",
    "TOP_P = 0.95  #@param {type:\"number\"}\n",
    "if int(BASE_SEED) == -1:\n",
    "    import random\n",
    "    BASE_SEED = random.randint(0, 2**31 - 1)\n",
    "    print(f\"BASE_SEED=-1 -> sampled random seed: {BASE_SEED}\")\n",
    "print(\n",
    "    f\"Unconditional run -> num={NUM_GENERATIONS}, base_seed={BASE_SEED}, variation_offset={VARIATION_OFFSET}, max_gen_len={MAX_GEN_LEN}, temperature={TEMPERATURE}, top_p={TOP_P}, lora={FINETUNED_PEFT_WEIGHT_PATH}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/Moonbeam-MIDI-Foundation-Model\n",
    "\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import io\n",
    "import contextlib\n",
    "import torch\n",
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "\n",
    "import sys\n",
    "\n",
    "# Avoid conflict with external `recipes` package (e.g., torchtune) by importing local generation.py directly.\n",
    "sys.path.insert(0, str(Path(\"recipes/inference/custom_music_generation\").resolve()))\n",
    "\n",
    "from generation import MusicLlama\n",
    "from llama_recipes.datasets.music_tokenizer import MusicTokenizer\n",
    "\n",
    "# Idempotent patch: avoid recursion when cell is rerun.\n",
    "if not hasattr(MusicTokenizer, \"_orig_convert_from_language_tokens\"):\n",
    "    MusicTokenizer._orig_convert_from_language_tokens = MusicTokenizer.convert_from_language_tokens\n",
    "\n",
    "def _convert_from_language_tokens_on_device(self, inp):\n",
    "    out = MusicTokenizer._orig_convert_from_language_tokens(self, inp)\n",
    "    return out.to(inp.device) if torch.is_tensor(inp) else out\n",
    "\n",
    "MusicTokenizer.convert_from_language_tokens = _convert_from_language_tokens_on_device\n",
    "\n",
    "\n",
    "def _normalize_checkpoint_state_dict(checkpoint: dict) -> dict:\n",
    "    if not isinstance(checkpoint, dict):\n",
    "        raise ValueError(\"Checkpoint must be a dict-like object.\")\n",
    "    if \"model_state_dict\" in checkpoint and isinstance(checkpoint[\"model_state_dict\"], dict):\n",
    "        state_dict = checkpoint[\"model_state_dict\"]\n",
    "    elif \"state_dict\" in checkpoint and isinstance(checkpoint[\"state_dict\"], dict):\n",
    "        state_dict = checkpoint[\"state_dict\"]\n",
    "    elif \"model\" in checkpoint and isinstance(checkpoint[\"model\"], dict):\n",
    "        state_dict = checkpoint[\"model\"]\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    return {k[7:] if k.startswith(\"module.\") else k: v for k, v in state_dict.items()}\n",
    "\n",
    "\n",
    "def _sanitize_tokens(tokenizer, rows):\n",
    "    sanitized_tokens = []\n",
    "    last_onset = 0\n",
    "    max_octave = max(0, tokenizer.octave_vocab_size - 3)\n",
    "    max_pitch = max(0, tokenizer.pitch_class_vocab_size - 3)\n",
    "    max_instrument = max(0, tokenizer.instrument_vocab_size - 3)\n",
    "    max_velocity = max(0, tokenizer.velocity_vocab_size - 3)\n",
    "    for row in rows:\n",
    "        if len(row) != 6:\n",
    "            continue\n",
    "        onset, duration, octave, pitch, instrument, velocity = [int(x) for x in row]\n",
    "        onset = max(0, onset)\n",
    "        duration = max(0, duration)\n",
    "        onset = max(onset, last_onset)\n",
    "        octave = min(max(0, octave), max_octave)\n",
    "        pitch = min(max(0, pitch), max_pitch)\n",
    "        instrument = min(max(0, instrument), max_instrument)\n",
    "        velocity = min(max(0, velocity), max_velocity)\n",
    "        sanitized_tokens.append([onset, duration, octave, pitch, instrument, velocity])\n",
    "        last_onset = onset\n",
    "    return sanitized_tokens\n",
    "\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA GPU is required for this Colab quickstart. In Colab, set Runtime -> Change runtime type -> GPU.\")\n",
    "\n",
    "# Prevent VRAM/system RAM buildup when rerunning cell with larger checkpoints (e.g. 839M).\n",
    "for var_name in [\"generator\", \"model\", \"checkpoint\", \"state_dict\", \"model_state\", \"filtered_state\", \"tokenizer\"]:\n",
    "    if var_name in globals():\n",
    "        del globals()[var_name]\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA GPU is required for this Colab quickstart. In Colab, set Runtime -> Change runtime type -> GPU.\")\n",
    "\n",
    "cfg_path = str(resolved_model_config_path)\n",
    "config = LlamaConfig.from_pretrained(cfg_path)\n",
    "model = LlamaForCausalLM(config)\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\", weights_only=True)\n",
    "except TypeError:\n",
    "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "state_dict = _normalize_checkpoint_state_dict(checkpoint)\n",
    "model_state = model.state_dict()\n",
    "filtered_state = {k: v for k, v in state_dict.items() if k in model_state and getattr(v, \"shape\", None) == model_state[k].shape}\n",
    "missing, unexpected = model.load_state_dict(filtered_state, strict=False)\n",
    "print(f\"[info] Loaded keys: {len(filtered_state)} | missing: {len(missing)} | unexpected: {len(unexpected)}\")\n",
    "if len(filtered_state) == 0:\n",
    "    raise RuntimeError(\"No checkpoint tensors matched model parameters. Check that resolved_model_config_path matches the checkpoint architecture.\")\n",
    "\n",
    "# Free large CPU objects as soon as possible.\n",
    "del checkpoint, state_dict, model_state, filtered_state\n",
    "gc.collect()\n",
    "\n",
    "if FINETUNED_PEFT_WEIGHT_PATH:\n",
    "    print(f\"Applying LoRA adapter from: {FINETUNED_PEFT_WEIGHT_PATH}\")\n",
    "    model = PeftModel.from_pretrained(model, FINETUNED_PEFT_WEIGHT_PATH, is_trainable=False)\n",
    "\n",
    "model = model.to(\"cuda\")\n",
    "if torch.cuda.is_bf16_supported():\n",
    "    model = model.to(torch.bfloat16)\n",
    "model.eval()\n",
    "\n",
    "# Suppress verbose tokenizer init print that can bloat Colab RAM/output history.\n",
    "with contextlib.redirect_stdout(io.StringIO()):\n",
    "    tokenizer = MusicTokenizer(\n",
    "        timeshift_vocab_size=config.onset_vocab_size,\n",
    "        dur_vocab_size=config.dur_vocab_size,\n",
    "        octave_vocab_size=config.octave_vocab_size,\n",
    "        pitch_class_vocab_size=config.pitch_class_vocab_size,\n",
    "        instrument_vocab_size=config.instrument_vocab_size,\n",
    "        velocity_vocab_size=config.velocity_vocab_size,\n",
    "    )\n",
    "\n",
    "generator = MusicLlama(model, tokenizer, config)\n",
    "\n",
    "num_generations = max(1, int(globals().get(\"NUM_GENERATIONS\", 1)))\n",
    "base_seed = int(globals().get(\"BASE_SEED\", 42))\n",
    "variation_offset = int(globals().get(\"VARIATION_OFFSET\", 0))\n",
    "max_gen_len = int(globals().get(\"MAX_GEN_LEN\", 256))\n",
    "temperature = float(globals().get(\"TEMPERATURE\", 0.9))\n",
    "top_p = float(globals().get(\"TOP_P\", 0.95))\n",
    "all_outputs = []\n",
    "\n",
    "for i in range(num_generations):\n",
    "    sample_seed = base_seed + variation_offset + i\n",
    "tokenizer = MusicTokenizer(\n",
    "    timeshift_vocab_size=config.onset_vocab_size,\n",
    "    dur_vocab_size=config.dur_vocab_size,\n",
    "    octave_vocab_size=config.octave_vocab_size,\n",
    "    pitch_class_vocab_size=config.pitch_class_vocab_size,\n",
    "    instrument_vocab_size=config.instrument_vocab_size,\n",
    "    velocity_vocab_size=config.velocity_vocab_size,\n",
    ")\n",
    "\n",
    "generator = MusicLlama(model, tokenizer, config)\n",
    "\n",
    "num_generations = globals().get(\"NUM_GENERATIONS\", 1)\n",
    "base_seed = globals().get(\"BASE_SEED\", 42)\n",
    "all_outputs = []\n",
    "\n",
    "for i in range(max(1, int(num_generations))):\n",
    "    sample_seed = base_seed + i\n",
    "    torch.manual_seed(sample_seed)\n",
    "    torch.cuda.manual_seed_all(sample_seed)\n",
    "\n",
    "    sos_prompt = [generator.tokenizer.sos_token_compound]\n",
    "    result = generator.music_completion(\n",
    "        prompt_tokens=[sos_prompt],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_gen_len=max_gen_len,\n",
    "    )[0]\n",
    "\n",
    "    sanitized_tokens = _sanitize_tokens(generator.tokenizer, result[\"generation\"][\"tokens\"])\n",
    "        temperature=0.9,\n",
    "        top_p=0.95,\n",
    "        max_gen_len=256,\n",
    "    )[0]\n",
    "\n",
    "    sanitized_tokens = []\n",
    "    last_onset = 0\n",
    "    max_octave = max(0, generator.tokenizer.octave_vocab_size - 3)\n",
    "    max_pitch = max(0, generator.tokenizer.pitch_class_vocab_size - 3)\n",
    "    max_instrument = max(0, generator.tokenizer.instrument_vocab_size - 3)\n",
    "    max_velocity = max(0, generator.tokenizer.velocity_vocab_size - 3)\n",
    "    for row in result[\"generation\"][\"tokens\"]:\n",
    "        if len(row) != 6:\n",
    "            continue\n",
    "        onset, duration, octave, pitch, instrument, velocity = [int(x) for x in row]\n",
    "        onset = max(0, onset)\n",
    "        duration = max(0, duration)\n",
    "        onset = max(onset, last_onset)\n",
    "        octave = min(max(0, octave), max_octave)\n",
    "        pitch = min(max(0, pitch), max_pitch)\n",
    "        instrument = min(max(0, instrument), max_instrument)\n",
    "        velocity = min(max(0, velocity), max_velocity)\n",
    "        sanitized_tokens.append([onset, duration, octave, pitch, instrument, velocity])\n",
    "        last_onset = onset\n",
    "\n",
    "    if not sanitized_tokens:\n",
    "        raise RuntimeError(f\"No valid generated tokens remained after sanitization for sample {i}.\")\n",
    "\n",
    "    out_path = Path(\"out.mid\") if num_generations == 1 else Path(f\"out_{i+1}.mid\")\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    generator.tokenizer.compound_to_midi(sanitized_tokens).save(str(out_path))\n",
    "    print(f\"Saved MIDI to: {out_path.resolve()} | sanitized_tokens={len(sanitized_tokens)} | seed={sample_seed}\")\n",
    "    all_outputs.append(out_path)\n",
    "\n",
    "print(f\"Done. Generated {len(all_outputs)} file(s).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6b) Continue from an uploaded MIDI\n",
    "Upload a MIDI, then generate one or more continuations with configurable length/seed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "continuation_controls"
   },
   "outputs": [],
   "source": [
    "# Continuation controls\n",
    "CONT_NUM_GENERATIONS = 2  #@param {type:\"integer\"}\n",
    "CONT_BASE_SEED = 123  #@param {type:\"integer\"}\n",
    "CONT_VARIATION_OFFSET = 0  #@param {type:\"integer\"}\n",
    "CONT_MAX_GEN_LEN = 192  #@param {type:\"integer\"}\n",
    "CONT_TEMPERATURE = 0.9  #@param {type:\"number\"}\n",
    "CONT_TOP_P = 0.95  #@param {type:\"number\"}\n",
    "CONT_USE_FULL_PROMPT = True  #@param {type:\"boolean\"}\n",
    "CONT_PROMPT_MAX_TOKENS = 256  #@param {type:\"integer\"}\n",
    "print(\n",
    "    f\"Continuation -> num={CONT_NUM_GENERATIONS}, base_seed={CONT_BASE_SEED}, variation_offset={CONT_VARIATION_OFFSET}, max_gen_len={CONT_MAX_GEN_LEN}, use_full_prompt={CONT_USE_FULL_PROMPT}, prompt_max_tokens={CONT_PROMPT_MAX_TOKENS}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "continuation_run"
   },
   "outputs": [],
   "source": [
    "%cd /content/Moonbeam-MIDI-Foundation-Model\n",
    "\n",
    "from pathlib import Path\n",
    "from google.colab import files\n",
    "import torch\n",
    "\n",
    "uploaded = files.upload()\n",
    "if not uploaded:\n",
    "    raise RuntimeError(\"No file uploaded. Upload a .mid file.\")\n",
    "\n",
    "upload_name = next(iter(uploaded.keys()))\n",
    "input_midi_path = Path(upload_name)\n",
    "print(f\"Uploaded: {input_midi_path}\")\n",
    "\n",
    "prompt_tokens = generator.tokenizer.midi_to_compound(str(input_midi_path))\n",
    "if not prompt_tokens:\n",
    "    raise RuntimeError(\"Uploaded MIDI produced an empty token list.\")\n",
    "\n",
    "use_full_prompt = bool(globals().get(\"CONT_USE_FULL_PROMPT\", True))\n",
    "prompt_max = max(1, int(globals().get(\"CONT_PROMPT_MAX_TOKENS\", 256)))\n",
    "if use_full_prompt:\n",
    "    prompt_tokens_for_gen = prompt_tokens\n",
    "else:\n",
    "    prompt_tokens_for_gen = prompt_tokens[-prompt_max:]\n",
    "\n",
    "num_generations = max(1, int(globals().get(\"CONT_NUM_GENERATIONS\", 1)))\n",
    "base_seed = int(globals().get(\"CONT_BASE_SEED\", 123))\n",
    "variation_offset = int(globals().get(\"CONT_VARIATION_OFFSET\", 0))\n",
    "max_gen_len = int(globals().get(\"CONT_MAX_GEN_LEN\", 192))\n",
    "temperature = float(globals().get(\"CONT_TEMPERATURE\", 0.9))\n",
    "top_p = float(globals().get(\"CONT_TOP_P\", 0.95))\n",
    "\n",
    "continuation_outputs = []\n",
    "for i in range(num_generations):\n",
    "    sample_seed = base_seed + variation_offset + i\n",
    "    torch.manual_seed(sample_seed)\n",
    "    torch.cuda.manual_seed_all(sample_seed)\n",
    "\n",
    "    result = generator.music_completion(\n",
    "        prompt_tokens=[prompt_tokens_for_gen],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_gen_len=max_gen_len,\n",
    "    )[0]\n",
    "\n",
    "    sanitized_tokens = _sanitize_tokens(generator.tokenizer, result[\"generation\"][\"tokens\"])\n",
    "    if not sanitized_tokens:\n",
    "        raise RuntimeError(f\"No valid continuation tokens remained after sanitization for sample {i}.\")\n",
    "\n",
    "    out_path = Path(f\"cont_{i+1}.mid\")\n",
    "    generator.tokenizer.compound_to_midi(sanitized_tokens).save(str(out_path))\n",
    "    print(f\"Saved continuation: {out_path.resolve()} | tokens={len(sanitized_tokens)} | seed={sample_seed}\")\n",
    "    continuation_outputs.append(out_path)\n",
    "\n",
    "print(f\"Used prompt tokens: {len(prompt_tokens_for_gen)} / original {len(prompt_tokens)}\")\n",
    "print(f\"Done. Generated {len(continuation_outputs)} continuation file(s).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Verify output and (optional) render to audio preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "outputs = sorted(Path('.').glob('out*.mid')) + sorted(Path('.').glob('cont_*.mid'))\n",
    "assert outputs, \"No MIDI outputs were created\"\n",
    "for out_path in outputs:\n",
    "    assert out_path.stat().st_size > 0, f\"{out_path} is empty\"\n",
    "print(\"\u2705 Generated files:\")\n",
    "outputs = sorted(Path('.').glob('out*.mid'))\n",
    "assert outputs, \"No MIDI outputs were created\"\n",
    "for out_path in outputs:\n",
    "    assert out_path.stat().st_size > 0, f\"{out_path} is empty\"\n",
    "print(\"âœ… Generated files:\")\n",
    "for out_path in outputs:\n",
    "    print(\" -\", out_path.resolve(), \"size:\", out_path.stat().st_size, \"bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional audio preview if dependencies are available.\n",
    "# If synthesis backends are unavailable in Colab, this cell may be skipped.\n",
    "\n",
    "!pip install pretty_midi midi2audio\n",
    "\n",
    "from pathlib import Path\n",
    "import pretty_midi\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "midi_files = sorted(Path('.').glob('out*.mid')) + sorted(Path('.').glob('cont_*.mid'))\n",
    "assert midi_files, \"No out*.mid/cont_*.mid files found. Run generation cell first.\"\n",
    "midi_files = sorted(Path('.').glob('out*.mid'))\n",
    "assert midi_files, \"No out*.mid files found. Run generation cell first.\"\n",
    "\n",
    "for midi_path in midi_files:\n",
    "    print(f\"Rendering: {midi_path}\")\n",
    "    midi = pretty_midi.PrettyMIDI(str(midi_path))\n",
    "    # Attempt software synthesis (requires fluidsynth backend in runtime)\n",
    "    audio = midi.synthesize(fs=16000)\n",
    "    display(Audio(audio, rate=16000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on checkpoint compatibility\n",
    "`MusicLlama.build()` in this fork was updated to accept multiple checkpoint layouts:\n",
    "- `{\"model_state_dict\": ...}`\n",
    "- `{\"state_dict\": ...}`\n",
    "- `{\"model\": ...}`\n",
    "- or a raw state dict\n",
    "\n",
    "It also strips `module.` prefixes when present."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}