{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moonbeam Quickstart (Google Colab, free GPU)\n",
    "\n",
    "This notebook runs **end-to-end inference** with the pretrained **Moonbeam 309M** checkpoint and writes `out.mid` with **no dataset and no finetuning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Runtime setup\n",
    "In Colab: `Runtime -> Change runtime type -> GPU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "print(subprocess.check_output([\"nvidia-smi\"], text=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Clone repo and install dependencies (exact README commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/guozixunnicolas/Moonbeam-MIDI-Foundation-Model.git\"\n",
    "REPO_DIR = \"/content/Moonbeam-MIDI-Foundation-Model\"\n",
    "\n",
    "if not os.path.isdir(REPO_DIR):\n",
    "    !git clone $REPO_URL $REPO_DIR\n",
    "else:\n",
    "    print(\"Repo already exists; syncing to latest origin/main (fallback origin/master).\")\n",
    "    !git -C $REPO_DIR fetch origin\n",
    "    !git -C $REPO_DIR reset --hard origin/main || git -C $REPO_DIR reset --hard origin/master\n",
    "\n",
    "%cd /content/Moonbeam-MIDI-Foundation-Model\n",
    "!pip install .\n",
    "!pip install src/llama_recipes/transformers_minimal/.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Download pretrained checkpoint from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "ckpt_path = hf_hub_download(\n",
    "    repo_id=\"guozixunnicolas/moonbeam-midi-foundation-model\",\n",
    "    filename=\"moonbeam_309M.pt\",\n",
    "    local_dir=\"checkpoints/pretrained\",\n",
    ")\n",
    "print(\"checkpoint:\", ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Resolve config + tokenizer paths used by repo\n",
    "- Model config: `src/llama_recipes/configs/model_config.json`\n",
    "- Tokenizer: search for `tokenizer.model` in repo, fallback to benchmark tokenizer path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import subprocess\n",
    "import torch\n",
    "\n",
    "repo_root = Path.cwd()\n",
    "primary_model_config_path = repo_root / \"src/llama_recipes/configs/model_config.json\"\n",
    "small_model_config_path = repo_root / \"src/llama_recipes/configs/model_config_small.json\"\n",
    "assert primary_model_config_path.exists(), f\"Missing model config: {primary_model_config_path}\"\n",
    "\n",
    "# Search for tokenizer.model in repo.\n",
    "search = subprocess.run(\n",
    "    [\"bash\", \"-lc\", \"rg --files | rg 'tokenizer\\.model$'\"],\n",
    "    cwd=repo_root,\n",
    "    text=True,\n",
    "    capture_output=True,\n",
    "    check=False,\n",
    ")\n",
    "found = [line.strip() for line in search.stdout.splitlines() if line.strip()]\n",
    "print(\"tokenizer.model candidates:\", found)\n",
    "\n",
    "if found:\n",
    "    tokenizer_path = repo_root / found[0]\n",
    "else:\n",
    "    tokenizer_path = repo_root / \"recipes/benchmarks/inference_throughput/tokenizer/tokenizer.model\"\n",
    "\n",
    "assert tokenizer_path.exists(), f\"Missing tokenizer file: {tokenizer_path}\"\n",
    "\n",
    "# Detect which config matches checkpoint tensor shapes (309M checkpoint expects *_small config).\n",
    "resolved_model_config_path = primary_model_config_path\n",
    "if 'ckpt_path' in globals() and small_model_config_path.exists():\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "    state = checkpoint.get('model_state_dict') if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint else checkpoint\n",
    "    if isinstance(state, dict):\n",
    "        norm_key = 'model.norm.weight'\n",
    "        ckpt_hidden_size = state.get(norm_key).shape[0] if norm_key in state else None\n",
    "        if ckpt_hidden_size is not None:\n",
    "            with open(primary_model_config_path) as f:\n",
    "                primary_hidden = json.load(f).get('hidden_size')\n",
    "            with open(small_model_config_path) as f:\n",
    "                small_hidden = json.load(f).get('hidden_size')\n",
    "            if ckpt_hidden_size == small_hidden and ckpt_hidden_size != primary_hidden:\n",
    "                resolved_model_config_path = small_model_config_path\n",
    "\n",
    "print(\"using model_config_path:\", resolved_model_config_path)\n",
    "print(\"using tokenizer_path:\", tokenizer_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Add dataset-free inference entrypoint (SOS-only prompt)\n",
    "This avoids the existing CSV + `.npy` prompt requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we are in the cloned repo (some Colab workflows can change cwd).\n",
    "%cd /content/Moonbeam-MIDI-Foundation-Model\n",
    "\n",
    "from pathlib import Path\n",
    "import textwrap\n",
    "\n",
    "entrypoint = Path(\"recipes/inference/custom_music_generation/unconditional_from_scratch.py\")\n",
    "entrypoint.parent.mkdir(parents=True, exist_ok=True)\n",
    "entrypoint.write_text(textwrap.dedent('''from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "\n",
    "from generation import MusicLlama\n",
    "from llama_recipes.datasets.music_tokenizer import MusicTokenizer\n",
    "\n",
    "\n",
    "def _normalize_checkpoint_state_dict(checkpoint: dict) -> dict:\n",
    "    if not isinstance(checkpoint, dict):\n",
    "        raise ValueError(\"Checkpoint must be a dict-like object.\")\n",
    "\n",
    "    if \"model_state_dict\" in checkpoint and isinstance(checkpoint[\"model_state_dict\"], dict):\n",
    "        state_dict = checkpoint[\"model_state_dict\"]\n",
    "    elif \"state_dict\" in checkpoint and isinstance(checkpoint[\"state_dict\"], dict):\n",
    "        state_dict = checkpoint[\"state_dict\"]\n",
    "    elif \"model\" in checkpoint and isinstance(checkpoint[\"model\"], dict):\n",
    "        state_dict = checkpoint[\"model\"]\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "\n",
    "    normalized = {}\n",
    "    for k, v in state_dict.items():\n",
    "        normalized[k[7:] if k.startswith(\"module.\") else k] = v\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def _resolve_model_config_path(ckpt_path: str, model_config_path: str) -> str:\n",
    "    model_config = Path(model_config_path)\n",
    "    small_config = model_config.with_name(\"model_config_small.json\")\n",
    "    if not small_config.exists():\n",
    "        return str(model_config)\n",
    "\n",
    "    try:\n",
    "        checkpoint = torch.load(ckpt_path, map_location=\"cpu\", weights_only=True)\n",
    "    except TypeError:\n",
    "        checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "    state_dict = _normalize_checkpoint_state_dict(checkpoint)\n",
    "    ckpt_hidden_size = state_dict.get(\"model.norm.weight\").shape[0] if \"model.norm.weight\" in state_dict else None\n",
    "    if ckpt_hidden_size is None:\n",
    "        return str(model_config)\n",
    "\n",
    "    cfg_hidden = LlamaConfig.from_pretrained(str(model_config)).hidden_size\n",
    "    small_hidden = LlamaConfig.from_pretrained(str(small_config)).hidden_size\n",
    "    if ckpt_hidden_size == small_hidden and ckpt_hidden_size != cfg_hidden:\n",
    "        print(f\"[info] Switching model config to checkpoint-compatible file: {small_config}\")\n",
    "        return str(small_config)\n",
    "    return str(model_config)\n",
    "\n",
    "\n",
    "def _build_music_llama(\n",
    "    ckpt_path: str,\n",
    "    model_config_path: str,\n",
    "    seed: int,\n",
    ") -> MusicLlama:\n",
    "    model_config_path = _resolve_model_config_path(ckpt_path, model_config_path)\n",
    "    config = LlamaConfig.from_pretrained(model_config_path)\n",
    "    model = LlamaForCausalLM(config)\n",
    "\n",
    "    try:\n",
    "        checkpoint = torch.load(ckpt_path, map_location=\"cpu\", weights_only=True)\n",
    "    except TypeError:\n",
    "        checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "    state_dict = _normalize_checkpoint_state_dict(checkpoint)\n",
    "    model_state = model.state_dict()\n",
    "    filtered_state = {\n",
    "        k: v for k, v in state_dict.items() if k in model_state and getattr(v, \"shape\", None) == model_state[k].shape\n",
    "    }\n",
    "    skipped = len(state_dict) - len(filtered_state)\n",
    "    missing, unexpected = model.load_state_dict(filtered_state, strict=False)\n",
    "    print(f\"[info] Loaded keys: {len(filtered_state)} | skipped: {skipped} | missing: {len(missing)} | unexpected: {len(unexpected)}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(\"cuda\")\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer = MusicTokenizer(\n",
    "        timeshift_vocab_size=config.onset_vocab_size,\n",
    "        dur_vocab_size=config.dur_vocab_size,\n",
    "        octave_vocab_size=config.octave_vocab_size,\n",
    "        pitch_class_vocab_size=config.pitch_class_vocab_size,\n",
    "        instrument_vocab_size=config.instrument_vocab_size,\n",
    "        velocity_vocab_size=config.velocity_vocab_size,\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
    "        model = model.to(torch.bfloat16)\n",
    "\n",
    "    return MusicLlama(model, tokenizer, config)\n",
    "\n",
    "\n",
    "def main(\n",
    "    ckpt_path: str,\n",
    "    model_config_path: str = \"src/llama_recipes/configs/model_config.json\",\n",
    "    tokenizer_path: str = \"recipes/benchmarks/inference_throughput/tokenizer/tokenizer.model\",\n",
    "    output_midi_path: str = \"out.mid\",\n",
    "    temperature: float = 0.9,\n",
    "    top_p: float = 0.95,\n",
    "    max_seq_len: int = 512,\n",
    "    max_gen_len: int = 256,\n",
    "    seed: int = 42,\n",
    "    finetuned_PEFT_weight_path: Optional[str] = None,\n",
    ") -> None:\n",
    "    \"\"\"Generate a MIDI file from an SOS-only prompt (no dataset required).\"\"\"\n",
    "    del tokenizer_path, max_seq_len, finetuned_PEFT_weight_path\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA GPU is required for this Colab quickstart. In Colab, set Runtime -> Change runtime type -> GPU.\")\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    generator = _build_music_llama(\n",
    "        ckpt_path=ckpt_path,\n",
    "        model_config_path=model_config_path,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    sos_prompt = [generator.tokenizer.sos_token_compound]\n",
    "    result = generator.music_completion(\n",
    "        prompt_tokens=[sos_prompt],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_gen_len=max_gen_len,\n",
    "    )[0]\n",
    "\n",
    "    output_path = Path(output_midi_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    result[\"generation\"][\"content\"].save(str(output_path))\n",
    "    print(f\"Saved MIDI to: {output_path.resolve()}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(main)\n",
    "'''))\n",
    "print(f\"Wrote/updated entrypoint: {entrypoint}\")\n",
    "assert \"_build_music_llama\" in entrypoint.read_text(), \"Entrypoint write failed; missing expected loader helper.\"\n",
    "\n",
    "!python recipes/inference/custom_music_generation/unconditional_from_scratch.py --help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Generate MIDI from scratch (no dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/Moonbeam-MIDI-Foundation-Model\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "\n",
    "import sys\n",
    "\n",
    "# Avoid conflict with external `recipes` package (e.g., torchtune) by importing local generation.py directly.\n",
    "sys.path.insert(0, str(Path(\"recipes/inference/custom_music_generation\").resolve()))\n",
    "\n",
    "from generation import MusicLlama\n",
    "from llama_recipes.datasets.music_tokenizer import MusicTokenizer\n",
    "\n",
    "\n",
    "def _normalize_checkpoint_state_dict(checkpoint: dict) -> dict:\n",
    "    if not isinstance(checkpoint, dict):\n",
    "        raise ValueError(\"Checkpoint must be a dict-like object.\")\n",
    "    if \"model_state_dict\" in checkpoint and isinstance(checkpoint[\"model_state_dict\"], dict):\n",
    "        state_dict = checkpoint[\"model_state_dict\"]\n",
    "    elif \"state_dict\" in checkpoint and isinstance(checkpoint[\"state_dict\"], dict):\n",
    "        state_dict = checkpoint[\"state_dict\"]\n",
    "    elif \"model\" in checkpoint and isinstance(checkpoint[\"model\"], dict):\n",
    "        state_dict = checkpoint[\"model\"]\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    return {k[7:] if k.startswith(\"module.\") else k: v for k, v in state_dict.items()}\n",
    "\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA GPU is required for this Colab quickstart. In Colab, set Runtime -> Change runtime type -> GPU.\")\n",
    "\n",
    "cfg_path = str(resolved_model_config_path)\n",
    "config = LlamaConfig.from_pretrained(cfg_path)\n",
    "model = LlamaForCausalLM(config)\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\", weights_only=True)\n",
    "except TypeError:\n",
    "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "state_dict = _normalize_checkpoint_state_dict(checkpoint)\n",
    "model_state = model.state_dict()\n",
    "filtered_state = {k: v for k, v in state_dict.items() if k in model_state and getattr(v, \"shape\", None) == model_state[k].shape}\n",
    "missing, unexpected = model.load_state_dict(filtered_state, strict=False)\n",
    "print(f\"[info] Loaded keys: {len(filtered_state)} | missing: {len(missing)} | unexpected: {len(unexpected)}\")\n",
    "\n",
    "model = model.to(\"cuda\")\n",
    "if torch.cuda.is_bf16_supported():\n",
    "    model = model.to(torch.bfloat16)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = MusicTokenizer(\n",
    "    timeshift_vocab_size=config.onset_vocab_size,\n",
    "    dur_vocab_size=config.dur_vocab_size,\n",
    "    octave_vocab_size=config.octave_vocab_size,\n",
    "    pitch_class_vocab_size=config.pitch_class_vocab_size,\n",
    "    instrument_vocab_size=config.instrument_vocab_size,\n",
    "    velocity_vocab_size=config.velocity_vocab_size,\n",
    ")\n",
    "\n",
    "generator = MusicLlama(model, tokenizer, config)\n",
    "sos_prompt = [generator.tokenizer.sos_token_compound]\n",
    "result = generator.music_completion(\n",
    "    prompt_tokens=[sos_prompt],\n",
    "    temperature=0.9,\n",
    "    top_p=0.95,\n",
    "    max_gen_len=256,\n",
    ")[0]\n",
    "\n",
    "out_path = Path(\"out.mid\")\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "result[\"generation\"][\"content\"].save(str(out_path))\n",
    "print(f\"Saved MIDI to: {out_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Verify output and (optional) render to audio preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "out_path = Path(\"out.mid\")\n",
    "assert out_path.exists() and out_path.stat().st_size > 0, \"out.mid was not created\"\n",
    "print(\"âœ… Generated:\", out_path.resolve(), \"size:\", out_path.stat().st_size, \"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional audio preview if dependencies are available.\n",
    "# If synthesis backends are unavailable in Colab, this cell may be skipped.\n",
    "\n",
    "!pip install pretty_midi midi2audio\n",
    "\n",
    "import pretty_midi\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "midi = pretty_midi.PrettyMIDI(\"out.mid\")\n",
    "# Attempt software synthesis (requires fluidsynth backend in runtime)\n",
    "audio = midi.synthesize(fs=16000)\n",
    "display(Audio(audio, rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on checkpoint compatibility\n",
    "`MusicLlama.build()` in this fork was updated to accept multiple checkpoint layouts:\n",
    "- `{\"model_state_dict\": ...}`\n",
    "- `{\"state_dict\": ...}`\n",
    "- `{\"model\": ...}`\n",
    "- or a raw state dict\n",
    "\n",
    "It also strips `module.` prefixes when present."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}