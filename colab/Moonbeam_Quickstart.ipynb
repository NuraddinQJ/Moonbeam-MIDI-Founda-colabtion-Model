{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moonbeam Quickstart (Google Colab, free GPU)\n",
    "\n",
    "This notebook runs **end-to-end inference** with the pretrained **Moonbeam 309M** checkpoint and writes `out.mid` with **no dataset and no finetuning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Runtime setup\n",
    "In Colab: `Runtime -> Change runtime type -> GPU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "print(subprocess.check_output([\"nvidia-smi\"], text=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Clone repo and install dependencies (exact README commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/guozixunnicolas/Moonbeam-MIDI-Foundation-Model.git\"\n",
    "REPO_DIR = \"/content/Moonbeam-MIDI-Foundation-Model\"\n",
    "\n",
    "if not os.path.isdir(REPO_DIR):\n",
    "    !git clone $REPO_URL $REPO_DIR\n",
    "else:\n",
    "    print(\"Repo already exists; syncing to latest origin/main (fallback origin/master).\")\n",
    "    !git -C $REPO_DIR fetch origin\n",
    "    !git -C $REPO_DIR reset --hard origin/main || git -C $REPO_DIR reset --hard origin/master\n",
    "\n",
    "%cd /content/Moonbeam-MIDI-Foundation-Model\n",
    "!pip install .\n",
    "!pip install src/llama_recipes/transformers_minimal/.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Download pretrained checkpoint from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "ckpt_path = hf_hub_download(\n",
    "    repo_id=\"guozixunnicolas/moonbeam-midi-foundation-model\",\n",
    "    filename=\"moonbeam_309M.pt\",\n",
    "    local_dir=\"checkpoints/pretrained\",\n",
    ")\n",
    "print(\"checkpoint:\", ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Resolve config + tokenizer paths used by repo\n",
    "- Model config: `src/llama_recipes/configs/model_config.json`\n",
    "- Tokenizer: search for `tokenizer.model` in repo, fallback to benchmark tokenizer path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "repo_root = Path.cwd()\n",
    "model_config_path = repo_root / \"src/llama_recipes/configs/model_config.json\"\n",
    "assert model_config_path.exists(), f\"Missing model config: {model_config_path}\"\n",
    "\n",
    "# Search for tokenizer.model in repo.\n",
    "search = subprocess.run(\n",
    "    [\"bash\", \"-lc\", \"rg --files | rg 'tokenizer\\.model$'\"],\n",
    "    cwd=repo_root,\n",
    "    text=True,\n",
    "    capture_output=True,\n",
    "    check=False,\n",
    ")\n",
    "found = [line.strip() for line in search.stdout.splitlines() if line.strip()]\n",
    "print(\"tokenizer.model candidates:\", found)\n",
    "\n",
    "if found:\n",
    "    tokenizer_path = repo_root / found[0]\n",
    "else:\n",
    "    tokenizer_path = repo_root / \"recipes/benchmarks/inference_throughput/tokenizer/tokenizer.model\"\n",
    "\n",
    "assert tokenizer_path.exists(), f\"Missing tokenizer file: {tokenizer_path}\"\n",
    "print(\"using model_config_path:\", model_config_path)\n",
    "print(\"using tokenizer_path:\", tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Add dataset-free inference entrypoint (SOS-only prompt)\n",
    "This avoids the existing CSV + `.npy` prompt requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we are in the cloned repo (some Colab workflows can change cwd).\n",
    "%cd /content/Moonbeam-MIDI-Foundation-Model\n",
    "\n",
    "from pathlib import Path\n",
    "import textwrap\n",
    "\n",
    "entrypoint = Path(\"recipes/inference/custom_music_generation/unconditional_from_scratch.py\")\n",
    "if not entrypoint.exists():\n",
    "    entrypoint.parent.mkdir(parents=True, exist_ok=True)\n",
    "    entrypoint.write_text(textwrap.dedent('''from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "\n",
    "from generation import MusicLlama\n",
    "\n",
    "\n",
    "def main(\n",
    "    ckpt_path: str,\n",
    "    model_config_path: str = \"src/llama_recipes/configs/model_config.json\",\n",
    "    tokenizer_path: str = \"recipes/benchmarks/inference_throughput/tokenizer/tokenizer.model\",\n",
    "    output_midi_path: str = \"out.mid\",\n",
    "    temperature: float = 0.9,\n",
    "    top_p: float = 0.95,\n",
    "    max_seq_len: int = 512,\n",
    "    max_gen_len: int = 256,\n",
    "    seed: int = 42,\n",
    "    finetuned_PEFT_weight_path: Optional[str] = None,\n",
    ") -> None:\n",
    "    \"\"\"Generate a MIDI file from an SOS-only prompt (no dataset required).\"\"\"\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    generator = MusicLlama.build(\n",
    "        ckpt_dir=ckpt_path,\n",
    "        model_config_path=model_config_path,\n",
    "        tokenizer_path=tokenizer_path,\n",
    "        max_seq_len=max_seq_len,\n",
    "        max_batch_size=1,\n",
    "        finetuned_PEFT_weight_path=finetuned_PEFT_weight_path,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    sos_prompt = [generator.tokenizer.sos_token_compound]\n",
    "    result = generator.music_completion(\n",
    "        prompt_tokens=[sos_prompt],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_gen_len=max_gen_len,\n",
    "    )[0]\n",
    "\n",
    "    output_path = Path(output_midi_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    result[\"generation\"][\"content\"].save(str(output_path))\n",
    "    print(f\"Saved MIDI to: {output_path.resolve()}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(main)\n",
    "'''))\n",
    "    print(f\"Created missing entrypoint: {entrypoint}\")\n",
    "else:\n",
    "    print(f\"Entrypoint already present: {entrypoint}\")\n",
    "\n",
    "!python recipes/inference/custom_music_generation/unconditional_from_scratch.py --help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Generate MIDI from scratch (no dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/Moonbeam-MIDI-Foundation-Model\n",
    "!python recipes/inference/custom_music_generation/unconditional_from_scratch.py   --ckpt_path \"$ckpt_path\"   --model_config_path src/llama_recipes/configs/model_config.json   --tokenizer_path \"$tokenizer_path\"   --output_midi_path out.mid   --max_seq_len 512   --max_gen_len 256   --temperature 0.9   --top_p 0.95\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Verify output and (optional) render to audio preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "out_path = Path(\"out.mid\")\n",
    "assert out_path.exists() and out_path.stat().st_size > 0, \"out.mid was not created\"\n",
    "print(\"âœ… Generated:\", out_path.resolve(), \"size:\", out_path.stat().st_size, \"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional audio preview if dependencies are available.\n",
    "# If synthesis backends are unavailable in Colab, this cell may be skipped.\n",
    "\n",
    "!pip install pretty_midi midi2audio\n",
    "\n",
    "import pretty_midi\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "midi = pretty_midi.PrettyMIDI(\"out.mid\")\n",
    "# Attempt software synthesis (requires fluidsynth backend in runtime)\n",
    "audio = midi.synthesize(fs=16000)\n",
    "display(Audio(audio, rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on checkpoint compatibility\n",
    "`MusicLlama.build()` in this fork was updated to accept multiple checkpoint layouts:\n",
    "- `{\"model_state_dict\": ...}`\n",
    "- `{\"state_dict\": ...}`\n",
    "- `{\"model\": ...}`\n",
    "- or a raw state dict\n",
    "\n",
    "It also strips `module.` prefixes when present."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}