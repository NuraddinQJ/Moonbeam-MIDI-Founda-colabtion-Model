{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moonbeam Quickstart (Google Colab, free GPU)\n",
    "\n",
    "This notebook runs **end-to-end inference** with the pretrained **Moonbeam 309M** checkpoint and writes `out.mid` with **no dataset and no finetuning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Runtime setup\n",
    "In Colab: `Runtime -> Change runtime type -> GPU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "print(subprocess.check_output([\"nvidia-smi\"], text=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Clone repo and install dependencies (exact README commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/guozixunnicolas/Moonbeam-MIDI-Foundation-Model.git\"\n",
    "REPO_DIR = \"/content/Moonbeam-MIDI-Foundation-Model\"\n",
    "\n",
    "if not os.path.isdir(REPO_DIR):\n",
    "    !git clone $REPO_URL $REPO_DIR\n",
    "else:\n",
    "    print(\"Repo already exists; syncing to latest origin/main (fallback origin/master).\")\n",
    "    !git -C $REPO_DIR fetch origin\n",
    "    !git -C $REPO_DIR reset --hard origin/main || git -C $REPO_DIR reset --hard origin/master\n",
    "\n",
    "%cd /content/Moonbeam-MIDI-Foundation-Model\n",
    "!pip install .\n",
    "!pip install src/llama_recipes/transformers_minimal/.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Download pretrained checkpoint from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "ckpt_path = hf_hub_download(\n",
    "    repo_id=\"guozixunnicolas/moonbeam-midi-foundation-model\",\n",
    "    filename=\"moonbeam_309M.pt\",\n",
    "    local_dir=\"checkpoints/pretrained\",\n",
    ")\n",
    "print(\"checkpoint:\", ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b) (Optional) Upload a LoRA adapter for generation\n",
    "If you trained a LoRA separately, upload a `.zip` containing `adapter_config.json` + adapter weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional LoRA upload controls\n",
    "USE_UPLOADED_LORA = False  #@param {type:\"boolean\"}\n",
    "UPLOADED_LORA_ZIP = \"\"  #@param {type:\"string\"}\n",
    "CLEAR_ACTIVE_LORA = False  #@param {type:\"boolean\"}\n",
    "\n",
    "# This variable controls which adapter is active for generation cells.\n",
    "if 'FINETUNED_PEFT_WEIGHT_PATH' not in globals():\n",
    "    FINETUNED_PEFT_WEIGHT_PATH = None\n",
    "\n",
    "if CLEAR_ACTIVE_LORA:\n",
    "    print(\"Cleared active LoRA. Generation will use base model only.\")\n",
    "elif USE_UPLOADED_LORA:\n",
    "    from google.colab import files\n",
    "    import zipfile\n",
    "    from pathlib import Path\n",
    "\n",
    "    if UPLOADED_LORA_ZIP.strip():\n",
    "        zip_path = Path(UPLOADED_LORA_ZIP)\n",
    "    else:\n",
    "        uploaded = files.upload()\n",
    "        if not uploaded:\n",
    "            raise RuntimeError(\"No LoRA zip uploaded.\")\n",
    "        zip_path = Path(next(iter(uploaded.keys())))\n",
    "\n",
    "    out_dir = Path(\"uploaded_lora\")\n",
    "    if out_dir.exists():\n",
    "        import shutil\n",
    "        shutil.rmtree(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "        zf.extractall(out_dir)\n",
    "\n",
    "    candidates = [d for d in [out_dir, *out_dir.rglob('*')] if d.is_dir() and (d / 'adapter_config.json').exists()]\n",
    "    if not candidates:\n",
    "        raise RuntimeError(\"Could not find adapter_config.json in uploaded LoRA zip.\")\n",
    "\n",
    "    FINETUNED_PEFT_WEIGHT_PATH = str(candidates[0].resolve())\n",
    "    print(f\"Active LoRA adapter set to: {FINETUNED_PEFT_WEIGHT_PATH}\")\n",
    "else:\n",
    "    print(\"No LoRA upload this run. Existing FINETUNED_PEFT_WEIGHT_PATH value will be used if set.\")\n",
    "\n",
    "print(f\"LoRA in effect for generation: {FINETUNED_PEFT_WEIGHT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Resolve config + tokenizer paths used by repo\n",
    "- Model config: `src/llama_recipes/configs/model_config.json`\n",
    "- Tokenizer: search for `tokenizer.model` in repo, fallback to benchmark tokenizer path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import subprocess\n",
    "import torch\n",
    "\n",
    "repo_root = Path.cwd()\n",
    "primary_model_config_path = repo_root / \"src/llama_recipes/configs/model_config.json\"\n",
    "small_model_config_path = repo_root / \"src/llama_recipes/configs/model_config_small.json\"\n",
    "assert primary_model_config_path.exists(), f\"Missing model config: {primary_model_config_path}\"\n",
    "\n",
    "# Search for tokenizer.model in repo.\n",
    "search = subprocess.run(\n",
    "    [\"bash\", \"-lc\", \"rg --files | rg 'tokenizer\\.model$'\"],\n",
    "    cwd=repo_root,\n",
    "    text=True,\n",
    "    capture_output=True,\n",
    "    check=False,\n",
    ")\n",
    "found = [line.strip() for line in search.stdout.splitlines() if line.strip()]\n",
    "print(\"tokenizer.model candidates:\", found)\n",
    "\n",
    "if found:\n",
    "    tokenizer_path = repo_root / found[0]\n",
    "else:\n",
    "    tokenizer_path = repo_root / \"recipes/benchmarks/inference_throughput/tokenizer/tokenizer.model\"\n",
    "\n",
    "assert tokenizer_path.exists(), f\"Missing tokenizer file: {tokenizer_path}\"\n",
    "\n",
    "# Detect which config matches checkpoint tensor shapes (309M checkpoint expects *_small config).\n",
    "resolved_model_config_path = primary_model_config_path\n",
    "if 'ckpt_path' in globals() and small_model_config_path.exists():\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "    state = checkpoint.get('model_state_dict') if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint else checkpoint\n",
    "    if isinstance(state, dict):\n",
    "        normalized_state = {k[7:] if k.startswith('module.') else k: v for k, v in state.items()}\n",
    "        norm_key = 'model.norm.weight'\n",
    "        ckpt_hidden_size = normalized_state.get(norm_key).shape[0] if norm_key in normalized_state else None\n",
    "        if ckpt_hidden_size is not None:\n",
    "            with open(primary_model_config_path) as f:\n",
    "                primary_hidden = json.load(f).get('hidden_size')\n",
    "            with open(small_model_config_path) as f:\n",
    "                small_hidden = json.load(f).get('hidden_size')\n",
    "            if ckpt_hidden_size == small_hidden and ckpt_hidden_size != primary_hidden:\n",
    "                resolved_model_config_path = small_model_config_path\n",
    "\n",
    "print(\"using model_config_path:\", resolved_model_config_path)\n",
    "print(\"using tokenizer_path:\", tokenizer_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Add dataset-free inference entrypoint (SOS-only prompt)\n",
    "This avoids the existing CSV + `.npy` prompt requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we are in the cloned repo (some Colab workflows can change cwd).\n",
    "%cd /content/Moonbeam-MIDI-Foundation-Model\n",
    "\n",
    "from pathlib import Path\n",
    "entrypoint = Path(\"recipes/inference/custom_music_generation/unconditional_from_scratch.py\")\n",
    "assert entrypoint.exists(), f\"Missing entrypoint: {entrypoint}\"\n",
    "print(f\"Using repo entrypoint (no notebook overwrite): {entrypoint}\")\n",
    "\n",
    "!PYTHONPATH=src python recipes/inference/custom_music_generation/unconditional_from_scratch.py --help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Generate MIDI from scratch (no dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: choose how many songs to generate\nSet `NUM_GENERATIONS` to 1, 2, 3, 4, etc., and rerun cell 6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User controls for cell 6 (unconditional generation)\n",
    "NUM_GENERATIONS = 4  #@param {type:\"integer\"}\n",
    "BASE_SEED = 42  #@param {type:\"integer\"}\n",
    "VARIATION_OFFSET = 0  #@param {type:\"integer\"}\n",
    "MAX_GEN_LEN = 256  #@param {type:\"integer\"}\n",
    "TEMPERATURE = 0.9  #@param {type:\"number\"}\n",
    "TOP_P = 0.95  #@param {type:\"number\"}\n",
    "if int(BASE_SEED) == -1:\n",
    "    import random\n",
    "    BASE_SEED = random.randint(0, 2**31 - 1)\n",
    "    print(f\"BASE_SEED=-1 -> sampled random seed: {BASE_SEED}\")\n",
    "print(\n",
    "    f\"Unconditional run -> num={NUM_GENERATIONS}, base_seed={BASE_SEED}, variation_offset={VARIATION_OFFSET}, max_gen_len={MAX_GEN_LEN}, temperature={TEMPERATURE}, top_p={TOP_P}, lora={FINETUNED_PEFT_WEIGHT_PATH}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/Moonbeam-MIDI-Foundation-Model\n",
    "\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, str(Path(\"recipes/inference/custom_music_generation\").resolve()))\n",
    "\n",
    "from unconditional_from_scratch import _build_music_llama, _sanitize_generated_tokens\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA GPU is required for this Colab quickstart.\")\n",
    "\n",
    "for var_name in [\"generator\", \"model\", \"tokenizer\"]:\n",
    "    if var_name in globals():\n",
    "        del globals()[var_name]\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "generator, load_summary, resolved_config_path, checkpoint_sig, config_sig = _build_music_llama(\n",
    "    ckpt_path=ckpt_path,\n",
    "    model_config_path=str(resolved_model_config_path),\n",
    "    seed=int(globals().get(\"BASE_SEED\", 42)),\n",
    "    finetuned_PEFT_weight_path=FINETUNED_PEFT_WEIGHT_PATH,\n",
    "    fail_fast=True,\n",
    "    max_allowed_load_issues=8,\n",
    ")\n",
    "\n",
    "print(\"[quickstart] active_lora:\", FINETUNED_PEFT_WEIGHT_PATH if FINETUNED_PEFT_WEIGHT_PATH else \"None (base model)\")\n",
    "print(\"[quickstart] resolved_config_path:\", resolved_config_path)\n",
    "print(\"[quickstart] load_summary:\", load_summary)\n",
    "\n",
    "num_generations = max(1, int(globals().get(\"NUM_GENERATIONS\", 1)))\n",
    "base_seed = int(globals().get(\"BASE_SEED\", 42))\n",
    "variation_offset = int(globals().get(\"VARIATION_OFFSET\", 0))\n",
    "max_gen_len = int(globals().get(\"MAX_GEN_LEN\", 256))\n",
    "temperature = float(globals().get(\"TEMPERATURE\", 0.9))\n",
    "top_p = float(globals().get(\"TOP_P\", 0.95))\n",
    "all_outputs = []\n",
    "\n",
    "for i in range(num_generations):\n",
    "    sample_seed = base_seed + variation_offset + i\n",
    "    torch.manual_seed(sample_seed)\n",
    "    torch.cuda.manual_seed_all(sample_seed)\n",
    "\n",
    "    result = generator.music_completion(\n",
    "        prompt_tokens=[[generator.tokenizer.sos_token_compound]],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_gen_len=max_gen_len,\n",
    "    )[0]\n",
    "\n",
    "    tokens = _sanitize_generated_tokens(result[\"generation\"][\"tokens\"], generator.tokenizer, enabled=True)\n",
    "    if not tokens:\n",
    "        raise RuntimeError(f\"No valid generated tokens remained for sample {i}.\")\n",
    "\n",
    "    out_path = Path(\"out.mid\") if num_generations == 1 else Path(f\"out_{i+1}.mid\")\n",
    "    generator.tokenizer.compound_to_midi(tokens).save(str(out_path))\n",
    "    print(f\"Saved MIDI: {out_path.resolve()} | tokens={len(tokens)} | seed={sample_seed}\")\n",
    "    all_outputs.append(out_path)\n",
    "\n",
    "print(f\"Done. Generated {len(all_outputs)} file(s).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6b) Continue from an uploaded MIDI\n",
    "Upload a MIDI, then generate one or more continuations with configurable length/seed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "continuation_controls"
   },
   "outputs": [],
   "source": [
    "# Continuation controls\n",
    "CONT_NUM_GENERATIONS = 2  #@param {type:\"integer\"}\n",
    "CONT_BASE_SEED = 123  #@param {type:\"integer\"}\n",
    "CONT_VARIATION_OFFSET = 0  #@param {type:\"integer\"}\n",
    "CONT_MAX_GEN_LEN = 192  #@param {type:\"integer\"}\n",
    "CONT_TEMPERATURE = 0.9  #@param {type:\"number\"}\n",
    "CONT_TOP_P = 0.95  #@param {type:\"number\"}\n",
    "CONT_USE_FULL_PROMPT = True  #@param {type:\"boolean\"}\n",
    "CONT_PROMPT_MAX_TOKENS = 256  #@param {type:\"integer\"}\n",
    "print(\n",
    "    f\"Continuation -> num={CONT_NUM_GENERATIONS}, base_seed={CONT_BASE_SEED}, variation_offset={CONT_VARIATION_OFFSET}, max_gen_len={CONT_MAX_GEN_LEN}, use_full_prompt={CONT_USE_FULL_PROMPT}, prompt_max_tokens={CONT_PROMPT_MAX_TOKENS}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "continuation_run"
   },
   "outputs": [],
   "source": [
    "%cd /content/Moonbeam-MIDI-Foundation-Model\n",
    "from pathlib import Path\n",
    "from google.colab import files\n",
    "import torch\n",
    "uploaded = files.upload()\n",
    "if not uploaded:\n",
    "    raise RuntimeError(\"No file uploaded. Upload a .mid file.\")\n",
    "upload_name = next(iter(uploaded.keys()))\n",
    "input_midi_path = Path(upload_name)\n",
    "print(f\"Uploaded: {input_midi_path}\")\n",
    "prompt_tokens = generator.tokenizer.midi_to_compound(str(input_midi_path))\n",
    "if not prompt_tokens:\n",
    "    raise RuntimeError(\"Uploaded MIDI produced an empty token list.\")\n",
    "use_full_prompt = bool(globals().get(\"CONT_USE_FULL_PROMPT\", True))\n",
    "prompt_max = max(1, int(globals().get(\"CONT_PROMPT_MAX_TOKENS\", 256)))\n",
    "if use_full_prompt:\n",
    "    prompt_tokens_for_gen = prompt_tokens\n",
    "else:\n",
    "    prompt_tokens_for_gen = prompt_tokens[-prompt_max:]\n",
    "num_generations = max(1, int(globals().get(\"CONT_NUM_GENERATIONS\", 1)))\n",
    "base_seed = int(globals().get(\"CONT_BASE_SEED\", 123))\n",
    "variation_offset = int(globals().get(\"CONT_VARIATION_OFFSET\", 0))\n",
    "max_gen_len = int(globals().get(\"CONT_MAX_GEN_LEN\", 192))\n",
    "temperature = float(globals().get(\"CONT_TEMPERATURE\", 0.9))\n",
    "top_p = float(globals().get(\"CONT_TOP_P\", 0.95))\n",
    "continuation_outputs = []\n",
    "for i in range(num_generations):\n",
    "    sample_seed = base_seed + variation_offset + i\n",
    "    torch.manual_seed(sample_seed)\n",
    "    torch.cuda.manual_seed_all(sample_seed)\n",
    "    result = generator.music_completion(\n",
    "        prompt_tokens=[prompt_tokens_for_gen],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_gen_len=max_gen_len,\n",
    "    )[0]\n",
    "    sanitized_tokens = _sanitize_generated_tokens(result[\"generation\"][\"tokens\"], generator.tokenizer, enabled=True)\n",
    "    if not sanitized_tokens:\n",
    "        raise RuntimeError(f\"No valid continuation tokens remained after sanitization for sample {i}.\")\n",
    "    out_path = Path(f\"cont_{i+1}.mid\")\n",
    "    generator.tokenizer.compound_to_midi(sanitized_tokens).save(str(out_path))\n",
    "    print(f\"Saved continuation: {out_path.resolve()} | tokens={len(sanitized_tokens)} | seed={sample_seed}\")\n",
    "    continuation_outputs.append(out_path)\n",
    "print(f\"Used prompt tokens: {len(prompt_tokens_for_gen)} / original {len(prompt_tokens)}\")\n",
    "print(f\"Done. Generated {len(continuation_outputs)} continuation file(s).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Verify output and (optional) render to audio preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "outputs = sorted(Path('.').glob('out*.mid')) + sorted(Path('.').glob('cont_*.mid'))\n",
    "assert outputs, \"No MIDI outputs were created\"\n",
    "for out_path in outputs:\n",
    "    assert out_path.stat().st_size > 0, f\"{out_path} is empty\"\n",
    "print(\"✅ Generated files:\")\n",
    "outputs = sorted(Path('.').glob('out*.mid'))\n",
    "assert outputs, \"No MIDI outputs were created\"\n",
    "for out_path in outputs:\n",
    "    assert out_path.stat().st_size > 0, f\"{out_path} is empty\"\n",
    "print(\"✅ Generated files:\")\n",
    "for out_path in outputs:\n",
    "    print(\" -\", out_path.resolve(), \"size:\", out_path.stat().st_size, \"bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional audio preview if dependencies are available.\n",
    "# If synthesis backends are unavailable in Colab, this cell may be skipped.\n",
    "\n",
    "!pip install pretty_midi midi2audio\n",
    "\n",
    "from pathlib import Path\n",
    "import pretty_midi\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "midi_files = sorted(Path('.').glob('out*.mid')) + sorted(Path('.').glob('cont_*.mid'))\n",
    "assert midi_files, \"No out*.mid/cont_*.mid files found. Run generation cell first.\"\n",
    "midi_files = sorted(Path('.').glob('out*.mid'))\n",
    "assert midi_files, \"No out*.mid files found. Run generation cell first.\"\n",
    "\n",
    "for midi_path in midi_files:\n",
    "    print(f\"Rendering: {midi_path}\")\n",
    "    midi = pretty_midi.PrettyMIDI(str(midi_path))\n",
    "    # Attempt software synthesis (requires fluidsynth backend in runtime)\n",
    "    audio = midi.synthesize(fs=16000)\n",
    "    display(Audio(audio, rate=16000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on checkpoint compatibility\n",
    "`MusicLlama.build()` in this fork was updated to accept multiple checkpoint layouts:\n",
    "- `{\"model_state_dict\": ...}`\n",
    "- `{\"state_dict\": ...}`\n",
    "- `{\"model\": ...}`\n",
    "- or a raw state dict\n",
    "\n",
    "It also strips `module.` prefixes when present."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
